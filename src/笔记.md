##  mapreduce

master :

### 我的实现：

map任务执行完后，才分配reduce任务，map不需要排序，reduce的worker读进所有的中间文件后，用Key进行排序再执行reduce任务。

worker通过RPC调用向master请求任务，完成任务后通过commit提交信息。所有的worker和master会在所有任务结束后退出。

1. 定时任务，即计时10s开在一个新goroutine里
2. worker可以无状态，只要task有状态，master就能管理了
3. 临时文件很重要，否则存在一种可能，worker文件写好后发送给master的commit延迟了，此时master因为超时将任务分给了一个新worker，然后收到了延迟了的commit会将task标记改为Done，但新开的worker却修改了之前的文件。所以产生这种问题原因是，文件操作在master看来应该原子化，不应该存在中间态，所以用临时文件写完整后再原子的Rename

## Raft

**课上讲的两个注意的bug：**

1. RPC调用时不要加锁，得想办法在RPC调用前把锁释放，不然两个持有锁的raft peer互相通信会死锁
2. 投票term需要检查，因为RPC调用时无锁，可能调用后原来期望的变量已经改变，因此需要检查对应变量，比如检查自己还是不是candidate以及当前的投票term。

**课上讲的Debug工具：**

1. go run -race xx
2. ctrl + / 打印调用栈(同时终止进程)，检查死锁工具
3. 一般都采用print打印

**论文博客阅读心得：**

1. raft论文的fig2 每一个都是必要的，不要去想当然，每一个流程甚至顺序都有其内在道理
2. fig8的不一致问题，通过规定这样的规则解决：leader只能提交自己任期的log entry或者连带提交前面的log entry。这个问题的产生是因为新leader没有拥有全部的commitable的log，通过这条规则解决。
3. commit的时机：满足2的规则并且majority server已经回复了Append RPC，确定了该log已经被其他server复制好，则直接commit该log。接着便可回复client
4. follower这些情况下会补给candidate投票：1.log不是up-to-data（这个的理解很重要，更新指的是candidate最后一个log 的 term 至少要和follower的相同，并且长度(index)还不能比follower长；2. candidate对应的term要大于follower的current term。都满足才会给你投票

**实现心得：**

1. 分布式编写，细节才是魔鬼，最大的困难一定是对原理的理解，比如对raft论文的深刻理解和仔细阅读，比如你身处某个状态(leader or follower)，在什么情况下会做什么事情，一定要细心的去实现
2. 次要困难是具体实现，比如RPC调用时加了锁造成某些情况下的死锁，又比如在一个连续的函数中，中间存在锁的释放，重新获得锁前没去检查之前的变量是否修改，造成难以发现的bug
3. 日志最好分几种，有非常详细的，也有不详细的
4. 分布式及其容易写bug，也极难排错，一定要想清楚每一步，比如锁有没有return是释放，会有哪些crash因素造成那些后果
   
   
**Raft 2A：**

* 好像没啥好说的，选举要快，一个任期只能一个leader，主要困难还是一开始对go语言的陌生
  
**Raft 2B:**
* 我快速找到冲突日志的方法，是客户直接把自己commit的地方发给leader，能一次就搞定，效率很高，代码逻辑也清晰的多; 

**Raft 2C:**
* 没什么难度，每次修改了需要持久的变量时，就persist()一下
* 跟论文不同-我把commitIndex变成持久变量，这样在crash重启时，可以不用重头开始发，显著减少网络流量

**Raft 2D**
* 难度很大，需要对日志压缩(快照)理解的很透彻才能写出正确的代码
  
  > 几乎整个代码全部检查修改了一遍，就因为logs结构变化导致全体改变，边界问题也是很麻烦

主要是清晰理解架构图：

>nil.csail.mit.edu/6.824/2021/notes/raft_diagram.pdf

**分层结构：**

Client <----> Service(State Machine) <----> Raft

基本逻辑：客户端的命令首先由Service层转发给Raft层，Raft层处理确认无误了（即raft节点commit对应命令后）,便提交给Service层，Service层将其应用到状态机里。在某个时刻(由Service层决定)创建一个当前状态机的快照，然后将快照信息的二进制文件发送给Raft层，告知已经将多少Index索引之前的logs写到了快照文件里了，Raft层便可以持久化它到磁盘，然后丢弃对应了冗余logs，即释放了内存。

启动阶段：Raft和Service层分别读快照，Raft将lastApplied变量设为快照对应的最后一个log索引，然后Raft之后只需向Service层提交快照之后的logs。


**注意事项：**

1. Raft层向Service层提交快照仅仅可能发生在Leader调用InstallSnapshot RPC时，即Leader发给了自己一个快照时。然后Service层收到此快照会调用CondInstallSnap通知Raft是否要应用快照，Raft检测其当前的情况，这个通知是不是过期的，如果是，则丢弃->我的实现保证了可以永远直接返回true,因为service层应用快照之前绝不会发新命令给Raft。
2. Leader绝不会向Service层发送快照信息ApplyMsg
3. 所有的日志索引位置都要改，写了个工具包raft_util，将global index和现在的index一一转换，这个工作极度伤神。
4. 测试2D-3出现了严重的死锁，经过长时间排查发现是有些return语句前忘记解锁

**代码重构：**

重构为三大板块对应四个小lab，其中持久化就和raft放一起了，重构后代码结构清晰很多。

**Raft 3A：**

在MIT6.824中实现的是强一致性模型，即读和写都是linerezability(线性化), 实现此模型是将所有client的请求都转发给Leader,显然此模型实现了fault-tolerant，但是甚至一定程度上的
降低的性能，Leader会成为所有信息的中心成为性能瓶颈。

如想用优化性能，只能牺牲强一致性，如Zookeeper一样，读请求可以发送给任何一个Server，但是会大概率读到过时的数据。在Zookeeper中保证了写的线性化，和Client意义上的FIFO模型。

每个Client自身只能串行访问server，即必须等待上一个请求返回才能发出下一个请求。

`lastRequestId`在每次对应Op已经应用到状态机时更新，所以就算服务器崩溃重启导致`lastRequestId`退回最后一次快照的状态也没关系。因为raft层会将log重新提交给service层，重新
应用到状态机，则`lastRequestId`也会重新更新到正确的位置。就算此时客户端发送了重复的信息，raft层虽然会replicate处理，但是也不会重复应用操作。

1. 所有命令，不管是读还是写，都经过了Raft层replicate处理后，再发给Service层，Service层再进行对应的Op操作；同样的，重复的命令也会重复经过Raft复制，然后上层再进行重复判断，通过LastRequestId
可以知道对应的client已经应用到状态机(Service层)的哪个位置，因此只要落后于这个位置就都是重复Op。

2. 在每次state machine应用log后更新对应client的最后应用的RequestId，表示此序号之前(包括此序号)的所有请求已经在kvDB上应用了，是重复的操作

3. `waitApplyCh   map[int]chan Op `为核心通信单元，每次Raft提交了一个log，Service层会将此log应用到状态机上，然后给此channel发送Op操作，表示已经执行完毕，客户端的RPC调用可以返回了。

> fix a bug，发现上层调用kill，service层也需要调用raft的kill。不然报错显示内存泄露。因为service层只有一个long time running goruntine，所以只需要一个地方killed()检测

**Raft 3B：**

service快照功能即在raft之前的基础上实现，即实现与raft的互动api，和重启时读入快照。
注意，service层重启时读快照，而raft层读raftState元数据，不需要将快照数据放入raft层，raft只需要知道service层的快照对应的最后一个logIndex。
在每次读到raft提交的一条log日志后检查当前日志占用空间，超过了阈值便生成快照发送给raft层，在raft层持久化后，raft层会通知service层应用快照。
同样的，当follower raft从leader那收到了一条快照，便会将其提交给上层，service将同样应用快照。

发现一个很坑的边界bug，因为每个raft节点都会独自的检查日志内存占用，达到某个阈值会各自的生成相应的snapshot，所以各个raft节点的snapshot不一定一样，
它们的LogIndex会边界一致，但snapshot边界不一致，因此有时leader发送AppendEntries RPC包时，follower会发现对应的preLog已经没了做成快照了。
我的实现是直接返回commitIndex以及false，leader就会正确的修改nextIndex位置了。

> 我🍀，我是傻逼。一个这种BUG改了我一个晚上，就是莫名其妙lab3B一个测试过不了，看了几十万条日志发现每次数据不一致之前都会伴随着一次Leader发的快照，反复检查代码“几万次”，发现是
> 个这种问题，如下 不用i，用了index，我真的佛了呜呜呜。
```go
	for i := index + 1; i <= rf.lastLogIndex(); i++ {
		tempLogs = append(tempLogs, rf.getLogWithIndex(index)) //这里是i 不是index，截断日志直接出错，使后面日志全部出错，最坑的是LAB2测试不出来，lab3B才测试出来
	}
```

分布式系统的DEBUG真的太太太困难了，分布式DEBUG难度>>多线程>>单线程，我现在mit6.824还是进程模拟终端，日志都在一起，如果是真正的分布式，DEBUG的难度可想而知。

* 一个细节，当follower收到leader的snapshot并应用到状态机上时，client是收不到回复的，因为service层已经丢失了对应的logIndex信息，因此会触发超时重新请求操作，但因为leader发给自己的snapshot带有lastRequestId所以可以知道它是重复操作。
* 快照Snapshot是包括KV数据库和lastRequestId两个Map组成，全部由raft层持久化，service层为快照创建者与提供者，raft层为管理者。

**DEBUG in 20220208**

过年回来后解决了一个较少情况会发生的bug，学会了写错误时才保存日志的脚本为`testFail.sh`。Bug为：比如当Raft已经提交log index到243给上层状态机了，状态机已经为243的状态，但此时收到了leader发的快照，快照尾部是242，我的程序会将这个快照发给上层。因为我的程序逻辑是上层收到快照都会应用到状态机，所以上层会应用这个快照，直接导致丢失了243的log。

因此这是一个raft层的bug，LAB2测试不出来，我修改了raft层提交快照给上层的逻辑，即必须commit index <= snapshotIndex才会发给上层应用。