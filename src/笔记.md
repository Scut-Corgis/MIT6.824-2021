##  mapreduce

master :

### 我的实现：

map任务执行完后，才分配reduce任务，map不需要排序，reduce的worker读进所有的中间文件后，用Key进行排序再执行reduce任务。

worker通过RPC调用向master请求任务，完成任务后通过commit提交信息。所有的worker和master会在所有任务结束后退出。

1. 定时任务，即计时10s开在一个新goroutine里
2. worker可以无状态，只要task有状态，master就能管理了
3. 临时文件很重要，否则存在一种可能，worker文件写好后发送给master的commit延迟了，此时master因为超时将任务分给了一个新worker，然后收到了延迟了的commit会将task标记改为Done，但新开的worker却修改了之前的文件。所以产生这种问题原因是，文件操作在master看来应该原子化，不应该存在中间态，所以用临时文件写完整后再原子的Rename

## Raft

**课上讲的两个注意的bug：**

1. RPC调用时不要加锁，得想办法在RPC调用前把锁释放，不然两个持有锁的raft peer互相通信会死锁
2. 投票term需要检查，因为RPC调用时无锁，可能调用后原来期望的变量已经改变，因此需要检查对应变量，比如检查自己还是不是candidate以及当前的投票term。

**课上讲的Debug工具：**

1. go run -race xx
2. ctrl + / 打印调用栈(同时终止进程)，检查死锁工具
3. 一般都采用print打印

**论文博客阅读心得：**

1. raft论文的fig2 每一个都是必要的，不要去想当然，每一个流程甚至顺序都有其内在道理
2. fig8的不一致问题，通过规定这样的规则解决：leader只能提交自己任期的log entry或者连带提交前面的log entry。这个问题的产生是因为新leader没有拥有全部的commitable的log，通过这条规则解决。
3. commit的时机：满足2的规则并且majority server已经回复了Append RPC，确定了该log已经被其他server复制好，则直接commit该log。接着便可回复client
4. follower这些情况下会补给candidate投票：1.log不是up-to-data（这个的理解很重要，更新指的是candidate最后一个log 的 term 至少要和follower的相同，并且长度(index)还不能比follower长；2. candidate对应的term要大于follower的current term。都满足才会给你投票

**实现心得：**

1. 分布式编写，细节才是魔鬼，最大的困难一定是对原理的理解，比如对raft论文的深刻理解和仔细阅读，比如你身处某个状态(leader or follower)，在什么情况下会做什么事情，一定要细心的去实现
2. 次要困难是具体实现，比如RPC调用时加了锁造成某些情况下的死锁，又比如在一个连续的函数中，中间存在锁的释放，重新获得锁前没去检查之前的变量是否修改，造成难以发现的bug
3. 日志最好分几种，有非常详细的，也有不详细的
4. 分布式及其容易写bug，也极难排错，一定要想清楚每一步，比如锁有没有return是释放，会有哪些crash因素造成那些后果
   
   
**Raft 2A：**

* 好像没啥好说的，选举要快，一个任期只能一个leader，主要困难还是一开始对go语言的陌生
  
**Raft 2B:**
* 我快速找到冲突日志的方法，是客户直接把自己commit的地方发给leader，能一次就搞定，效率很高，代码逻辑也清晰的多; 

**Raft 2C:**
* 没什么难度，每次修改了需要持久的变量时，就persist()一下
* 跟论文不同-我把commitIndex变成持久变量，这样在crash重启时，可以不用重头开始发，显著减少网络流量

**Raft 2D**
* 难度很大，需要对日志压缩(快照)理解的很透彻才能写出正确的代码
  
  > 几乎整个代码全部检查修改了一遍，就因为logs结构变化导致全体改变，边界问题也是很麻烦

主要是清晰理解架构图：

>nil.csail.mit.edu/6.824/2021/notes/raft_diagram.pdf

**分层结构：**

Client <----> Service(State Machine) <----> Raft

基本逻辑：客户端的命令首先由Service层转发给Raft层，Raft层处理确认无误了（即raft节点commit对应命令后）,便提交给Service层，Service层将其应用到状态机里。在某个时刻(由Service层决定)创建一个当前状态机的快照，然后将快照信息的二进制文件发送给Raft层，告知已经将多少Index索引之前的logs写到了快照文件里了，Raft层便可以持久化它到磁盘，然后丢弃对应了冗余logs，即释放了内存。

启动阶段：Raft和Service层分别读快照，Raft将lastApplied变量设为快照对应的最后一个log索引，然后Raft之后只需向Service层提交快照之后的logs。


**注意事项：**

1. Raft层向Service层提交快照仅仅可能发生在Leader调用InstallSnapshot RPC时，即Leader发给了自己一个快照时。然后Service层收到此快照会调用CondInstallSnap通知Raft是否要应用快照，Raft检测其当前的情况，这个通知是不是过期的，如果是，则丢弃->我的实现保证了可以永远直接返回true,因为service层应用快照之前绝不会发新命令给Raft。
2. Leader绝不会向Service层发送快照信息ApplyMsg
3. 所有的日志索引位置都要改，写了个工具包raft_util，将global index和现在的index一一转换，这个工作极度伤神。
4. 测试2D-3出现了严重的死锁，经过长时间排查发现是有些return语句前忘记解锁

```go
//定时器到时会向对应的timer.C发送一个数据，此时调用Stop会返回false，因此此时应该将channel里的数据读出来，否则会影响下一次定时。
//再者，Reset定时器前必须Stop停止它。
func (rf *Raft) resetTimer() {
	rf.timerLock.Lock()
	//timer的使用手法，好像没有别的正确的方法了，不能修改下面的任何代码
	if !rf.timer.Stop() {
		select {
		case <-rf.timer.C:
		default:
		}
	}
	duration := time.Duration(rand.Int())%electionTimeoutInterval + electionTimeoutStart
	rf.timer.Reset(duration)
	rf.timerLock.Unlock()
}
```
**代码重构：**

重构为三大板块对应四个小lab，其中持久化就和raft放一起了，重构后代码结构清晰很多。

**Raft 3A：**

在MIT6.824中实现的是强一致性模型，即读和写都是linerezability(线性化), 实现此模型是将所有client的请求都转发给Leader,显然此模型实现了fault-tolerant，但是甚至一定程度上的
降低的性能，Leader会成为所有信息的中心成为性能瓶颈。

如想用优化性能，只能牺牲强一致性，如Zookeeper一样，读请求可以发送给任何一个Server，但是会大概率读到过时的数据。在Zookeeper中保证了写的线性化，和Client意义上的FIFO模型。

每个Client自身只能串行访问server，即必须等待上一个请求返回才能发出下一个请求。

`lastRequestId`在每次对应Op已经应用到状态机时更新，所以就算服务器崩溃重启导致`lastRequestId`退回最后一次快照的状态也没关系。因为raft层会将log重新提交给service层，重新
应用到状态机，则`lastRequestId`也会重新更新到正确的位置。就算此时客户端发送了重复的信息，raft层虽然会replicate处理，但是也不会重复应用操作。

1. 所有命令，不管是读还是写，都经过了Raft层replicate处理后，再发给Service层，Service层再进行对应的Op操作；同样的，重复的命令也会重复经过Raft复制，然后上层再进行重复判断，通过LastRequestId
可以知道对应的client已经应用到状态机(Service层)的哪个位置，因此只要落后于这个位置就都是重复Op。

>  读也要记录log，是防止这种不一致情况：网络分区导致出现两边都有leader，若client之前先发送了一个写请求被成功执行，然后client发送读请求，理论上必须能看到写请求结果，但是若此时发送给了小分区的leader，它会直接回复你旧的记录，因为导致无法强一致性。而如果记录了读log，该小分区的leader无法将此log分发给大部分raft结点因此无法提交log，也就不会回复client。

> 先走raft，再走重复逻辑是出于以下情况：若前一个log正在走raft层复制，而此时来了一个重复的log，如果之间去重会忽略掉或直接回复等等，但这是危险的，因为底层raft结点共识可能会失败，导致日志丢失，因此都先走raft保证大多数结点都看见都拥有这份日志后再去重，才能保证安全性。

2. 在每次state machine应用log后更新对应client的最后应用的RequestId，表示此序号之前(包括此序号)的所有请求已经在kvDB上应用了，是重复的操作

3. `waitApplyCh   map[int]chan Op `为核心通信单元，每次Raft提交了一个log，Service层会将此log应用到状态机上，然后给此channel发送Op操作，表示已经执行完毕，客户端的RPC调用可以返回了。

> fix a bug，发现上层调用kill，service层也需要调用raft的kill。不然报错显示内存泄露。因为service层只有一个long time running goruntine，所以只需要一个地方killed()检测

**Raft 3B：**

service快照功能即在raft之前的基础上实现，即实现与raft的互动api，和重启时读入快照。
注意，service层重启时读快照，而raft层读raftState元数据，不需要将快照数据放入raft层，raft只需要知道service层的快照对应的最后一个logIndex。
在每次读到raft提交的一条log日志后检查当前日志占用空间，超过了阈值便生成快照发送给raft层，在raft层持久化后，raft层会通知service层应用快照。
同样的，当follower raft从leader那收到了一条快照，便会将其提交给上层，service将同样应用快照。

发现一个很坑的边界bug，因为每个raft节点都会独自的检查日志内存占用，达到某个阈值会各自的生成相应的snapshot，所以各个raft节点的snapshot不一定一样，
它们的LogIndex会边界一致，但snapshot边界不一致，因此有时leader发送AppendEntries RPC包时，follower会发现对应的preLog已经没了做成快照了。
我的实现是直接返回commitIndex以及false，leader就会正确的修改nextIndex位置了。

> 我🍀，我是傻逼。一个这种BUG改了我一个晚上，就是莫名其妙lab3B一个测试过不了，看了几十万条日志发现每次数据不一致之前都会伴随着一次Leader发的快照，反复检查代码“几万次”，发现是
> 个这种问题，如下 不用i，用了index，我真的佛了呜呜呜。
```go
	for i := index + 1; i <= rf.lastLogIndex(); i++ {
		tempLogs = append(tempLogs, rf.getLogWithIndex(index)) //这里是i 不是index，截断日志直接出错，使后面日志全部出错，最坑的是LAB2测试不出来，lab3B才测试出来
	}
```

分布式系统的DEBUG真的太太太困难了，分布式DEBUG难度>>多线程>>单线程，我现在mit6.824还是进程模拟终端，日志都在一起，如果是真正的分布式，DEBUG的难度可想而知。

* 一个细节，当follower收到leader的snapshot并应用到状态机上时，client是收不到回复的，因为service层已经丢失了对应的logIndex信息，因此会触发超时重新请求操作，但因为leader发给自己的snapshot带有lastRequestId所以可以知道它是重复操作。
* 快照Snapshot是包括KV数据库和lastRequestId两个Map组成，全部由raft层持久化，service层为快照创建者与提供者，raft层为管理者。

**DEBUG in 20220208**

过年回来后解决了一个较少情况会发生的bug，学会了写错误时才保存日志的脚本为`testFail.sh`。Bug为：比如当Raft已经提交log index到243给上层状态机了，状态机已经为243的状态，但此时收到了leader发的快照，快照尾部是242，我的程序会将这个快照发给上层。因为我的程序逻辑是上层收到快照都会应用到状态机，所以上层会应用这个快照，直接导致丢失了243的log。

因此这是一个raft层的bug，LAB2测试不出来，我修改了raft层提交快照给上层的逻辑，即必须commit index <= snapshotIndex才会发给上层应用。

**Lab 4A**

基本实现和lab3没区别，多了一个负载均衡，实际上Lab4A就是实现管理员Administrator和shardctrler的集群通信,从而可以修改shardctrler上的集群配置信息。

https://blog.csdn.net/qq_40443651/article/details/118034894

**Lab 4B**

`shardkv：` snapshot、数据库层面的put、append、get代码逻辑不变，引入了两个新的raft log index类型，即
NewConfig 和 MIGRATESHARDOp。 

因为NewConfig 和 MIGRATESHARDOp是服务端之间的信息，没有ClientId 和 RequestId去重，所以实际是通过configId来表达是否执行过此Op。

lab4 的snapshot包括各分片组件情况，config以及迁移数组情况。

config逻辑：
相对于lab3 service层增加两个long running goroutine，一个间歇轮询 ctrler 询问是否config改变，另一个间歇遍历迁移bool数组。

流程：
1. G1 leader询问得到config改变，为了一致性，先交给raft层共识，直到raft commit 此ConfigOp表示已经共识完毕
2. 更改当前config，并检查现在的config是否有需要迁移的 Shard Component， 将对应的分片标记为正在迁移，则对应的分片在迁移完毕前将拒绝client访问。
3. 接着第三个long running goroutine则会检查到了需要迁移，它就会制作好需要迁移的一致数据单元。注意本实现是push分片出去，即将不属于自己的shard通过RPC args发送给对应的group集群G2，
4. G2集群leader收到后，会先将其作为日志给raft层共识，raft commit给上层后，执行该MigrateOp, 即更新数据库接收对应分片，并重置迁移数组为false，最终才会返回G1的RPC调用表示已经成功迁移。
5. G1 RPC成功返回后，将此迁移信息给raft层共识，共识完成后，迁移数组对应发送的分片位置重置为false，表示发出去的分片迁移已经完成。所以push方在本实现中是慢于接受方修改config的。

一些细节：

* 数据库的客户端client拥有一个ctrler的指针，但只能执行其`Query()`方法，用于询问当前config。
* 数据库的客户端client的视角看来是强一致性的，即对他来说是看不见复杂的分布式架构，数据库行为和单机数据库一致
* lab3中 服务器只有一个集群，所有的外界消息都是通过leader处理，所以实际上只实现了FT(容错),反而降低了性能，因为leader还需要共识才能回复客户端
* lab4 引入分片数据库，从而实现了真正的客户端命令并行处理，理论上shard的集群越多，性能越高。
* 与分布式事务设计的coordinator不同， coordinator主要是实现两阶段提交和两阶段锁(锁的管理), 本lab4的控制者shardctrler用于管理集群配置，即不同的分片置于哪一个集群下，以及管理集群的进入与退出，当然也包括负载均衡。但值得注意的是，shardctrler不参与迁移过程，它只提供元数据即最终分片的分布情况，各集群得到分片情况后自己RPC沟通迁移分片。
* 互相迁移分片的两个集群config号码永远不会相差2，因为相差1时push的一方得不到回应，即迁移bool数组拥有会有至少一个分片正在迁移，本实现中只要有分片正在迁移，便不会更新config，因此分片的push方和get方会互相等待，直到push方将分片成功转发过去，然后get方才能重置迁移数组增加config，然后push方RPC调用返回重置迁移数组，之后才可以继续增加config。
* 两个交换分配的group他们config必须一致。换句话说，肯定会先有一个集群读了ctrler的新config先增加了，那么它发送分片push RPC会得到错误的返回，然后它会一直重试，直到对方也读到了ctrler的新config才会开始交换分片。

一些问题：

**push分片而不采取pull？**

无法保证安全性，或者说为了安全会增加额外的RPC通信以及使代码更复杂。
若采用pull，A的leader向B的leader发起pull RPC，包含着对应需要拉的分片信息，此时B RPC调用返回对应分片之前必须告知其follower，比如删除对应分片的log，当大多数结点得到此log后，B提交此log并返回分片然后删除对应分片。但问题是A还没有安全收到分片，那么B理论上不能删除分片，但问题是如果不此时删除分配，B将再无机会删除分片，触发A再返回一个ACK告知你提交log，但是raft层逻辑不应被应用层侵入式的修改，raft层是独立的对上层透明的。又或者A发起新的RPC告知你删除分片，这又会导致一个额外的RPC消耗，也会增加代码的复杂程度。

而push的方法，A完全可以直接push，B在收到此RPC携带的分片后先不回复A，而是通过raft共识给follower后再回复A，然后就可以应用对应的分配到状态机上了。此时A收到B的回复，A便可以安全的将删除这个命令生成一个log散发给自己的follower来共识并最终删除follower，整个阶段只需一次RPC通信。当然也有可能A发送分片给B后A就崩溃了，其实没有关系，因此A恢复后会再次尝试发送分片，而B会检测到重复命令因此会直接回复OK成功，所以说push方法解决了分片迁移的问题。

TODO： LAB4B debug 
> 注： 可用kvraft我写的`testFail.sh`脚本精准打印错误日志
```go
hjg@ubuntu:~/go/src/6.824/src/shardkv$ grep -n "FAIL" 100次测试20220216.log 
320:--- FAIL: TestUnreliable2 (7.27s)
333:FAIL
335:FAIL        6.824/shardkv   142.806s
535:--- FAIL: TestUnreliable2 (9.26s)
548:FAIL
550:FAIL        6.824/shardkv   153.728s
600:--- FAIL: TestUnreliable2 (10.54s)
613:FAIL
615:FAIL        6.824/shardkv   173.347s
849:--- FAIL: TestChallenge1Delete (22.67s)
857:FAIL
859:FAIL        6.824/shardkv   168.560s
969:--- FAIL: TestUnreliable2 (10.63s)
982:FAIL
984:FAIL        6.824/shardkv   161.647s
1030:--- FAIL: TestConcurrent3 (23.40s)
1047:FAIL
1049:FAIL       6.824/shardkv   159.906s
1249:--- FAIL: TestUnreliable2 (8.40s)
1262:FAIL
1264:FAIL       6.824/shardkv   152.559s
2274:--- FAIL: TestUnreliable2 (12.43s)
2287:FAIL
2289:FAIL       6.824/shardkv   139.234s
2545:--- FAIL: TestConcurrent3 (27.47s)
2553:--- FAIL: TestUnreliable2 (7.98s)
2566:FAIL
2568:FAIL       6.824/shardkv   140.773s
```